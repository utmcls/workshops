{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b052d7ad",
   "metadata": {},
   "source": [
    "# Translating Meowian\n",
    "\n",
    "A cardboard box has appeared at your front door. In it, there is a shivering cat with a PS Vita around its neck. As you bring in the cat to your warm home, it insistently meows at you and text in Latin letters pops up on the screen. It seems to be translating the cat's meowing! After fidgeting with the bootlegged PS Vita, you notice that, while the audio to text functionality works, meowian_txt_to_english.py is partially corrupt. It's up to you to recreate the Meowian-to-English translator to understand what this cat is telling you!\n",
    "\n",
    "## On Meowian\n",
    "\n",
    "From perusing the files contained in the PS Vita, you come to learn that Meowian is a language spoken by all cats, but which has many dialects and varieties. Your new cat is a worldly cat and thus speaks many different types of Meowian, including Informal Meowian and Formal Meowian.\n",
    "\n",
    "In this folder, you have two text files, informal_meowian.txt and formal_meowian.txt, each containing transcriptions of the cat's output in those respective varieties. There is also a meowian-english.csv file containing a parallel of Meowian words and their English translation. The developer was even kind enough to leave an English meowian_report.txt file which describes the structure of Meowian and some of its varieties.\n",
    "\n",
    "## Task\n",
    "\n",
    "In order to successfully translate one of the Meowian text files into English, your job is to fix the code below by completing the TODOs. Informal Meowian is easier as its structure is more similar to that of English, but if you want a challenge, you can try your hand at translating the more complex Formal Meowian!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2c94a1",
   "metadata": {},
   "source": [
    "## 1. Preprocessing the Corpus\n",
    "\n",
    "Your first step is preprocessing the Meowian-English aligned corpus. Preprocessing is when you prepare the data before the main operation. This can include cleaning the data of unwanted details, lowercasing, removing stop words(\"the\", \"and\", \"or\" etc.), splitting sentences into words, and more!\n",
    "\n",
    "First, let's extract the data from the parallel corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0a2e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "tokenized_english = []\n",
    "tokenized_meowian = []\n",
    "\n",
    "file = pd.read_csv(\"meowian-english.csv\")\n",
    "english_sentences = file['english'].tolist() # Take the English data\n",
    "meowian_sentences = None # TODO: Take the Meowian data\n",
    "\n",
    "print(english_sentences)\n",
    "print(meowian_sentences)\n",
    "\n",
    "all_sentences = [english_sentences, meowian_sentences]\n",
    "all_tokenized = [tokenized_english, tokenized_meowian]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82616499",
   "metadata": {},
   "source": [
    "Now, let's tokenize the sentence. Tokenization is the process of breaking text into smaller units called tokens. Depending on the system, a token may represent a word, a subword, or even punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72a7b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "for i in range(len(all_sentences)):\n",
    "    for sentence in all_sentences[i]:\n",
    "        #TODO: use nltk.word_tokenize, then add it to the appropriate list of tokenized items!\n",
    "        tokenized_sentence = None #fix this\n",
    "        all_tokenized[i].extend(tokenized_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd7b0d4",
   "metadata": {},
   "source": [
    "## 2. Model Training\n",
    "\n",
    "We can now start setting up our models and training them. If you're unfamiliar with this work, start with the Informal Meowian section which uses an IBM Model 1. If you want a challenge, skip to implementing a Transformer for Formal Meowian!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26520f3",
   "metadata": {},
   "source": [
    "## A. Informal Meowian and IBM Model 1\n",
    "\n",
    "IBM Model 1 is one of a family of statistical machine translation models from the early 1990s which were at the forefront of machine translation until neural network models gained popularity. To illustrate how IBM Model 1 works in practice, the following code trains the model on a small parallel corpus. Each pair of sentences is wrapped in an AlignedSent object, and then the model is trained for 10 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e41313",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate import AlignedSent, IBMModel1\n",
    "\n",
    "aligned_sentences = []\n",
    "\n",
    "for source, target in zip(tokenized_meowian, tokenized_english):\n",
    "        aligned_sentences.append(AlignedSent(source.split(), target.split()))\n",
    "        ibm_model = IBMModel1(aligned_sentences, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb641c6e",
   "metadata": {},
   "source": [
    "Training IBM Model 1 is that simple! Now, we must preprocess the text to be translated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77838811",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_input = []\n",
    "translated_words = []\n",
    "\n",
    "# TODO: Extract the input. Remember read_csv from the first preprocessing step? You can also use it on text files.\n",
    "input = None \n",
    "input = input.to_string()\n",
    "\n",
    "# TODO: Clean the output, similarly as before.\n",
    "for sentence in nltk.sent_tokenize(input):\n",
    "        tokenized_sentence = None # TODO: use nltk.word_tokenize\n",
    "        tokenized_input.extend(tokenized_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffe811e",
   "metadata": {},
   "source": [
    "Now to actually translating!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4818156f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for source_word in tokenized_input:\n",
    "    max_prob = 0.0\n",
    "    translated_word = None\n",
    "    for target_word in ibm_model.translation_table[source_word]:\n",
    "        prob = ibm_model.translation_table[source_word][target_word]\n",
    "        if prob > max_prob:\n",
    "            max_prob = prob\n",
    "            translated_word = target_word\n",
    "    if translated_word is not None:\n",
    "        translated_words.append(translated_word)\n",
    "translated_text = ' '.join(translated_words)\n",
    "\n",
    "print(translated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8748ea",
   "metadata": {},
   "source": [
    "Wahoo! We've successfully translated Informal Meowian to English! Now, go back and try running the code on Formal Meowian. What do you notice?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96416cd3",
   "metadata": {},
   "source": [
    "## B. Formal Meowian and RNNs\n",
    "\n",
    "Since IBM Model 1 is rather primitive, ignoring word order among other things, we'll use a more recent type of model to deal with Formal Meowian: Recurrent Neural Networks! RNNs process sequences one element at a time, allowing them to capture ordering and contextual relationships that statistical models like IBM Model 1 simply can’t represent.\n",
    "\n",
    "First, let's build a tokenizer for each language. A tokenizer, well, tokenizes, as we did before, except these ones are special: they turn the strings into number representations that our model will be able to handle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f46baaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense, RepeatVector, TimeDistributed\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# First, we create a Tokenizer object\n",
    "meowian_tokenizer = Tokenizer(oov_token=\"<UNK>\") # sets unknown vocab to be UNK\n",
    "# Fit the tokenizer to the sentences\n",
    "meowian_tokenizer.fit_on_texts(meowian_sentences)\n",
    "# Turn sentences into number representations (sequences)\n",
    "meowian_sequences = meowian_tokenizer.texts_to_sequences(meowian_sentences)\n",
    "meowian_max_len = max(len(seq) for seq in meowian_sequences)\n",
    " # Make sequences all the same length\n",
    "meowian_x = pad_sequences(meowian_sequences, maxlen=meowian_max_len, padding='post')\n",
    "meowian_vocab_size = len(meowian_tokenizer.word_index) + 1\n",
    "\n",
    "# TODO: Do the same for English!\n",
    "en_tokenizer = None # TODO: create Tokenizer object\n",
    "#TODO: fit the tokenizer\n",
    "en_sequences = None # TODO: turn sentences into sequences\n",
    "english_max_len = None # TODO: find max length\n",
    "en_y = None # TODO: pad sequences\n",
    "en_vocab_size = None # TODO: find vocab size\n",
    "\n",
    "print(f\"\\nMeowian vocabulary: {meowian_tokenizer.word_index}\")\n",
    "print(f\"English vocabulary: {en_tokenizer.word_index}\")\n",
    "print(f\"\\nPadded Meowian sequences shape: {meowian_x.shape}\")\n",
    "print(f\"Padded English sequences shape: {en_y.shape}\")\n",
    "print(f\"\\nMeowian vocab size: {meowian_vocab_size}\")\n",
    "print(f\"English vocab size: {en_vocab_size}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea03ea0",
   "metadata": {},
   "source": [
    "We can finally build our model! It will be made up of:\n",
    "    1. An embedding layer\n",
    "    2. An encoder\n",
    "    3. A repeat vector\n",
    "    4. A decoder\n",
    "    5. An output layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a279cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    # Input layer: Embedding to convert word indices to dense vectors\n",
    "    Embedding(\n",
    "        input_dim=meowian_vocab_size,\n",
    "        output_dim=8,  # Tiny embedding size for our tiny dataset\n",
    "        input_length=meowian_max_len,\n",
    "        mask_zero=True,  # Ignore padding\n",
    "        name=\"embedding_layer\"\n",
    "    ),\n",
    "    \n",
    "    # RNN layer: Processes the sequence\n",
    "    SimpleRNN(\n",
    "        units=8,  # Very small for our tiny dataset\n",
    "        activation='tanh',\n",
    "        return_sequences=False,  # Only output at the end\n",
    "        name=\"rnn_layer\"\n",
    "    ),\n",
    "    \n",
    "    # Repeat the RNN output for each time step in output\n",
    "    RepeatVector(english_max_len, name=\"repeat_layer\"),\n",
    "    \n",
    "    # Another RNN for decoding\n",
    "    SimpleRNN(\n",
    "        units=8,\n",
    "        activation='tanh',\n",
    "        return_sequences=True,  # Output at each time step\n",
    "        name=\"decoder_rnn\"\n",
    "    ),\n",
    "    \n",
    "    # Output layer: Predict English word at each position\n",
    "    TimeDistributed(\n",
    "        Dense(en_vocab_size, activation='softmax'),\n",
    "        name=\"output_layer\"\n",
    "    )\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',  # Use sparse since we have integer labels\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Show model architecture\n",
    "model.build(input_shape=(None, meowian_max_len))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf25c4b3",
   "metadata": {},
   "source": [
    "Training time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392d16ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    meowian_x,\n",
    "    en_y,  # en_y is already the right shape (batch_size, sequence_length)\n",
    "    epochs=200,\n",
    "    batch_size=1,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8decb4c",
   "metadata": {},
   "source": [
    "Now to actual translating!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6e116d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_with_rnn(text, model, meowian_tokenizer, en_tokenizer, max_len):\n",
    "    \"\"\"Translate Meowian to English using the trained RNN\"\"\"\n",
    "    \n",
    "    # Clean and tokenize input\n",
    "    text = text.strip()\n",
    "    \n",
    "    # Convert to sequence\n",
    "    sequence = meowian_tokenizer.texts_to_sequences([text])\n",
    "    \n",
    "    # Pad sequence\n",
    "    padded = pad_sequences(sequence, maxlen=max_len, padding='post')\n",
    "    \n",
    "    # Get prediction\n",
    "    predictions = model.predict(padded, verbose=0)[0]  # Shape: (english_max_len, en_vocab_size)\n",
    "    \n",
    "    # Convert predictions to words\n",
    "    translated_words = []\n",
    "    for time_step in predictions:\n",
    "        # Get most likely word index\n",
    "        word_idx = np.argmax(time_step)\n",
    "        \n",
    "        # Skip padding (0) and unknown (<UNK>)\n",
    "        if word_idx > 0 and word_idx <= len(en_tokenizer.word_index):\n",
    "            # Find word for this index\n",
    "            for word, idx in en_tokenizer.word_index.items():\n",
    "                if idx == word_idx:\n",
    "                    translated_words.append(word)\n",
    "                    break\n",
    "    \n",
    "    return ' '.join(translated_words) if translated_words else \"[No translation]\"\n",
    "\n",
    "translate_with_rnn(\"mew mewe mewew\", model, meowian_tokenizer, en_tokenizer, meowian_max_len)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a679492",
   "metadata": {},
   "source": [
    "Wahoo! It works! Now try generating Meowian sentences of your own by combining words from meowian-english.csv and put them in the translator. What do you notice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ac7266",
   "metadata": {},
   "outputs": [],
   "source": [
    "your_meowian_sentence = \"\"\n",
    "english_translation = \"\"\n",
    "\n",
    "result = translate_with_rnn(your_meowian_sentence, model, meowian_tokenizer, en_tokenizer, meowian_max_len)\n",
    "\n",
    "print(your_meowian_sentence)\n",
    "print(english_translation, result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab47986",
   "metadata": {},
   "source": [
    "Wah! The cat’s output happened to be something it already saw during training. That doesn’t tell us whether it actually learned anything, so we still need to test it on unseen data to evaluate real generalization. But for now, let's translate formal_meowian.txt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176e6f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Extract formal_meowian.txt\n",
    "input = None\n",
    "input = input.to_string()\n",
    "\n",
    "#call the translate_with_rnn function on the input and print the result\n",
    "result = None #TODO call the function\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b77628c",
   "metadata": {},
   "source": [
    "This gives us a rough idea of how the model behaves on new text. Proper evaluation will have to wait for another adventure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8612957",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "At last, the PS Vita hums to life with a working Meowian‑to‑English translator. The cat curls up on your lap, finally able to tell you exactly what it’s been meowing about this whole time. Our workshop ends here, but your adventures in feline linguistics are only just beginning.\n",
    "\n",
    "Congratulations on completing the workshop! You’ve explored statistical models, neural models, and the full pipeline of building a translation system."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
